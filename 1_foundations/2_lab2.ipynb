{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to the Second Lab - Week 1, Day 3\n",
    "\n",
    "Today we will work with lots of models! This is a way to get comfortable with APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/stop.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">Important point - please read</h2>\n",
    "            <span style=\"color:#ff7800;\">The way I collaborate with you may be different to other courses you've taken. I prefer not to type code while you watch. Rather, I execute Jupyter Labs, like this, and give you an intuition for what's going on. My suggestion is that you carefully execute this yourself, <b>after</b> watching the lecture. Add print statements to understand what's going on, and then come up with your own variations.<br/><br/>If you have time, I'd love it if you submit a PR for changes in the community_contributions folder - instructions in the resources. Also, if you have a Github account, use this to showcase your variations. Not only is this essential practice, but it demonstrates your skills to others, including perhaps future clients or employers...\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with imports - ask ChatGPT to explain any package that you don't know\n",
    "\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from anthropic import Anthropic\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Always remember to do this!\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anthropic API Key exists and begins sk-ant-\n",
      "Google API Key exists and begins AI\n",
      "xAI API Key exists and begins xai\n"
     ]
    }
   ],
   "source": [
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "# openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "# deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "# groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "xai_api_key = os.getenv('XAI_API_KEY')\n",
    "\n",
    "# if openai_api_key:\n",
    "#     print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "# else:\n",
    "#     print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set (and this is optional)\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:2]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set (and this is optional)\")\n",
    "\n",
    "# if deepseek_api_key:\n",
    "#     print(f\"DeepSeek API Key exists and begins {deepseek_api_key[:3]}\")\n",
    "# else:\n",
    "#     print(\"DeepSeek API Key not set (and this is optional)\")\n",
    "\n",
    "# if groq_api_key:\n",
    "#     print(f\"Groq API Key exists and begins {groq_api_key[:4]}\")\n",
    "# else:\n",
    "#     print(\"Groq API Key not set (and this is optional)\")\n",
    "\n",
    "if xai_api_key:\n",
    "    print(f\"xAI API Key exists and begins {xai_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"xAI API Key not set (and this is optional)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "competitors = []\n",
    "answers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = \"Please come up with a challenging, nuanced question that I can ask a number of LLMs to evaluate their intelligence. \"\n",
    "request += \"Answer only with the question, no explanation.\"\n",
    "messages = [{\"role\": \"user\", \"content\": request}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': 'Please come up with a challenging, nuanced question that I can ask a number of LLMs to evaluate their intelligence. Answer only with the question, no explanation.'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You discover that a highly advanced alien civilization has been observing Earth for centuries and has concluded that humanity's greatest achievement isn't a technological invention or artistic masterpiece, but rather a specific cognitive habit or way of thinking that emerged from our particular evolutionary and cultural development. What might this habit be, and why would it be more valuable to them than our tangible accomplishments?\n"
     ]
    }
   ],
   "source": [
    "# openai = OpenAI()\n",
    "# response = openai.chat.completions.create(\n",
    "#     model=\"gpt-4o-mini\",\n",
    "#     messages=messages,\n",
    "# )\n",
    "# question = response.choices[0].message.content\n",
    "\n",
    "# claude = Anthropic()\n",
    "# response = claude.messages.create(model=\"claude-sonnet-4-5-20250929\", messages=messages, max_tokens=1000)\n",
    "# question = response.content[0].text\n",
    "# print(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"i have a gaming laptop and want to run the best ollama model as possible on its gpu with 16gb vram (rtx 3080 mobile ) what is the best model with what quantization\"\n",
    "messages = [{\"role\": \"user\", \"content\": question}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The API we know well\n",
    "\n",
    "# model_name = \"gpt-4o-mini\"\n",
    "\n",
    "# response = openai.chat.completions.create(model=model_name, messages=messages)\n",
    "# answer = response.choices[0].message.content\n",
    "\n",
    "# display(Markdown(answer))\n",
    "# competitors.append(model_name)\n",
    "# answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Best Ollama Models for RTX 3080 Mobile (16GB VRAM)\n",
       "\n",
       "With 16GB VRAM, you have several excellent options. Here are my recommendations:\n",
       "\n",
       "## Top Choices:\n",
       "\n",
       "### **1. Llama 3.1 70B Q2_K (Best Overall)**\n",
       "```bash\n",
       "ollama run llama3.1:70b-instruct-q2_K\n",
       "```\n",
       "- **VRAM usage:** ~14-15GB\n",
       "- **Quality:** Excellent reasoning despite Q2 quantization\n",
       "- **Why:** The larger parameter count (70B) often outperforms smaller models even at lower quantization\n",
       "\n",
       "### **2. Qwen 2.5 32B Q4_K_M (Balanced)**\n",
       "```bash\n",
       "ollama run qwen2.5:32b-instruct-q4_K_M\n",
       "```\n",
       "- **VRAM usage:** ~13-14GB\n",
       "- **Quality:** Outstanding performance for coding and reasoning\n",
       "- **Why:** Better quantization (Q4) with very capable 32B model\n",
       "\n",
       "### **3. Llama 3.1 70B Q3_K_M (If you want higher quality)**\n",
       "```bash\n",
       "ollama run llama3.1:70b-instruct-q3_K_M\n",
       "```\n",
       "- **VRAM usage:** ~15.5-16GB (tight fit)\n",
       "- **Quality:** Noticeably better than Q2\n",
       "- **Why:** Pushes your VRAM limit but worth it\n",
       "\n",
       "### **4. Mistral Small 22B Q5_K_M (Speed + Quality)**\n",
       "```bash\n",
       "ollama run mistral-small:22b-instruct-2409-q5_K_M\n",
       "```\n",
       "- **VRAM usage:** ~11-12GB\n",
       "- **Quality:** Excellent quality with high quantization\n",
       "- **Why:** Fast inference, great quality, headroom for other apps\n",
       "\n",
       "## My Recommendation:\n",
       "Start with **Qwen 2.5 32B Q4_K_M** - it offers the best balance of quality, speed, and VRAM efficiency for most tasks, especially coding.\n",
       "\n",
       "## Quick Tips:\n",
       "- Close other applications to maximize available VRAM\n",
       "- Monitor temps on mobile GPU (they can throttle)\n",
       "- Use `ollama ps` to check actual VRAM usage\n",
       "\n",
       "What will you primarily use the model for? I can refine the recommendation based on your use case."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Anthropic has a slightly different API, and Max Tokens is required\n",
    "\n",
    "model_name = \"claude-sonnet-4-5-20250929\"\n",
    "\n",
    "claude = Anthropic()\n",
    "response = claude.messages.create(model=model_name, messages=messages, max_tokens=1000)\n",
    "answer = response.content[0].text\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "That's a fantastic setup for local AI! Your **RTX 3080 mobile with 16GB VRAM** is a sweet spot, allowing you to run very capable models entirely on your GPU.\n",
       "\n",
       "The \"best\" model is always a balance of **quality, speed, and VRAM usage**. With 16GB, you have a good amount to play with.\n",
       "\n",
       "Here's what I recommend, along with the ideal quantizations:\n",
       "\n",
       "---\n",
       "\n",
       "### Understanding Quantization (`Q_K_M`):\n",
       "\n",
       "*   **Quantization (Q)** reduces the precision of the model's weights, making it smaller and faster, but with a potential slight loss in quality.\n",
       "*   **`_K_M` (e.g., Q5_K_M)**: This is the modern, optimized quantization scheme for GGUF (the format Ollama uses). It offers a much better balance of speed and quality compared to older quantizations.\n",
       "*   **Higher number = Less compression = Better quality / More VRAM / Slower.**\n",
       "*   **Lower number = More compression = Lower quality / Less VRAM / Faster.**\n",
       "\n",
       "**General Recommendation for your 16GB VRAM:**\n",
       "You'll generally want to aim for **Q5_K_M** or **Q6_K** for 7B/8B models, and **Q4_K_M** or **Q5_K_M** for 13B models.\n",
       "\n",
       "---\n",
       "\n",
       "### Top Model Recommendations for your 16GB VRAM:\n",
       "\n",
       "1.  **Llama 3 8B (Recommended - Best Overall Balance)**\n",
       "    *   **Why:** Llama 3 is Meta's latest and greatest. The 8B version is incredibly capable for its size, offering excellent reasoning, instruction following, and general knowledge. It's often considered state-of-the-art for its parameter count.\n",
       "    *   **Quantization:**\n",
       "        *   **`llama3:8b-instruct-q6_K` (around 5.8GB VRAM)**: Excellent balance of quality and speed. This is probably the sweet spot.\n",
       "        *   **`llama3:8b-instruct-q5_K_M` (around 5.1GB VRAM)**: Slightly smaller, very good quality, and slightly faster.\n",
       "        *   **`llama3:8b-instruct-q8_0` (around 8.2GB VRAM)**: You can even run this for maximum quality on the 8B model, though the difference from q6_K is often minor.\n",
       "    *   **How to run:** `ollama run llama3:8b-instruct-q6_K`\n",
       "\n",
       "2.  **Mistral 7B (Excellent Alternative - Fast & Creative)**\n",
       "    *   **Why:** Mistral 7B is known for its speed, creativity, and good performance on a wide range of tasks. It's a fantastic all-rounder and a bit faster than Llama 3 8B in some cases. Many popular fine-tunes are based on Mistral.\n",
       "    *   **Quantization:**\n",
       "        *   **`mistral:7b-instruct-v0.2-q6_K` (around 5.0GB VRAM)**: Great balance.\n",
       "        *   **`mistral:7b-instruct-v0.2-q5_K_M` (around 4.4GB VRAM)**: Very fast and still high quality.\n",
       "    *   **How to run:** `ollama run mistral:7b-instruct-v0.2-q6_K`\n",
       "\n",
       "3.  **OpenHermes 2.5 (Fine-tuned for Chat/Instruction Following)**\n",
       "    *   **Why:** This is a fine-tune of Mistral 7B, specifically optimized for chat and instruction following. It's often praised for its conversational abilities and willingness to follow complex prompts.\n",
       "    *   **Quantization:**\n",
       "        *   **`openhermes2.5-mistral:7b-q6_K` (around 5.0GB VRAM)**: Ideal for quality.\n",
       "        *   **`openhermes2.5-mistral:7b-q5_K_M` (around 4.4GB VRAM)**: Still very good and zippy.\n",
       "    *   **How to run:** `ollama run openhermes2.5-mistral:7b-q6_K`\n",
       "\n",
       "4.  **Llama 2 13B (For More \"Brainpower\" - Pushing VRAM)**\n",
       "    *   **Why:** If you want a bit more raw capability than a 7B/8B model and don't mind a slight speed reduction, a 13B model can offer more complex reasoning. While Llama 3 8B often beats Llama 2 13B, there are still scenarios where the larger parameter count can shine.\n",
       "    *   **Quantization:**\n",
       "        *   **`llama2:13b-chat-q5_K_M` (around 8.5GB VRAM)**: This will fit comfortably and offer good quality.\n",
       "        *   **`llama2:13b-chat-q4_K_M` (around 7.5GB VRAM)**: A good option if you want to save a little VRAM or prefer slightly faster generation, with minimal perceived quality loss.\n",
       "    *   **How to run:** `ollama run llama2:13b-chat-q5_K_M`\n",
       "\n",
       "---\n",
       "\n",
       "### What to AVOID (for full GPU offload):\n",
       "\n",
       "*   **Mixtral 8x7B (or any similar \"expert\" model):** While Mixtral is excellent, its \"8x7B\" architecture means it effectively uses weights equivalent to a much larger model (around 45-50B parameters) when all experts are considered. Even a Q4_K_M version of Mixtral is typically ~25-30GB, far exceeding your 16GB VRAM for full GPU offload. It will run very slowly with significant CPU offload.\n",
       "\n",
       "---\n",
       "\n",
       "### How to Get Started:\n",
       "\n",
       "1.  **Install Ollama:** If you haven't already, download and install Ollama from [ollama.com](https://ollama.com/).\n",
       "2.  **Download and Run:** Open your terminal or command prompt and use the `ollama run` command for the model and quantization you want. Ollama will automatically download it if you don't have it.\n",
       "    *   Example: `ollama run llama3:8b-instruct-q6_K`\n",
       "3.  **Monitor VRAM:** While running, open `NVIDIA-smi` in another terminal window to monitor your GPU's VRAM usage.\n",
       "\n",
       "### Pro Tip: Experiment!\n",
       "\n",
       "Download a couple of these and try them out. You might find you prefer the \"personality\" or specific strengths of one over another for your particular use cases. Start with Llama 3 8B Q6_K, as it's currently the go-to for many.\n",
       "\n",
       "Enjoy your powerful local AI setup!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gemini = OpenAI(api_key=google_api_key, base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")\n",
    "model_name = \"gemini-2.5-flash\"\n",
    "\n",
    "response = gemini.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deepseek = OpenAI(api_key=deepseek_api_key, base_url=\"https://api.deepseek.com/v1\")\n",
    "# model_name = \"deepseek-chat\"\n",
    "\n",
    "# response = deepseek.chat.completions.create(model=model_name, messages=messages)\n",
    "# answer = response.choices[0].message.content\n",
    "\n",
    "# display(Markdown(answer))\n",
    "# competitors.append(model_name)\n",
    "# answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groq = OpenAI(api_key=groq_api_key, base_url=\"https://api.groq.com/openai/v1\")\n",
    "# model_name = \"llama-3.3-70b-versatile\"\n",
    "\n",
    "# response = groq.chat.completions.create(model=model_name, messages=messages)\n",
    "# answer = response.choices[0].message.content\n",
    "\n",
    "# display(Markdown(answer))\n",
    "# competitors.append(model_name)\n",
    "# answers.append(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Based on your setup (gaming laptop with RTX 3080 mobile and 16GB VRAM), I'll recommend the best Ollama model in terms of capability (i.e., intelligence, reasoning, and output quality) that you can run effectively on your GPU. Ollama uses CUDA for NVIDIA GPU acceleration, and it supports automatic layer offloading—if the model doesn't fully fit in VRAM, it loads as many layers as possible onto the GPU and falls back to CPU/RAM for the rest. This still gives a big performance boost over pure CPU.\n",
       "\n",
       "### Key Considerations for Your Hardware\n",
       "- **VRAM Limits**: 16GB is solid, but large models (e.g., 70B+ parameters) won't fit entirely in VRAM even when quantized (compressed). Expect ~10-40 layers on GPU depending on the model and quantization (with the rest on CPU). This can yield 5-15 tokens/second generation speed, which is usable for most tasks.\n",
       "- **Quantization**: Lower quantization (e.g., Q4) reduces model size to fit more on GPU but slightly lowers quality. Higher quantization (e.g., Q8) preserves quality but requires more VRAM. I prioritize a balance of quality, size, and performance.\n",
       "- **Other Factors**: Your laptop's CPU (assuming something like an Intel i7 or Ryzen 7) and at least 16GB system RAM will help with offloading. Keep context size reasonable (e.g., 4K-8K tokens) to avoid VRAM overflow. Use Ollama's latest version for optimal GPU support.\n",
       "- **Testing**: Results can vary by exact laptop model (cooling, power limits). Start with `ollama run <model>` and monitor VRAM usage with tools like `nvidia-smi`.\n",
       "\n",
       "### Recommended Model: Llama 3.1 70B with Q4_K_M Quantization\n",
       "- **Why this model?** Llama 3.1 70B is one of the most capable open-source models available in Ollama right now—excellent for complex reasoning, coding, writing, and general tasks. It's significantly smarter than smaller models like 7B/8B or 13B variants, even if it runs slower due to partial offloading. For 16GB VRAM, this is the \"best\" (most advanced) you can realistically run without constant swapping or crashes. Alternatives like Mixtral 8x22B or Llama 3.1 405B are too big even with heavy quantization.\n",
       "- **Quantization: Q4_K_M**\n",
       "  - **Why Q4_K_M?** It's a great balance: model size is ~40GB (fits ~25-35 layers on your 16GB GPU, depending on context). Quality is high (close to unquantized) with minimal loss compared to Q3 or Q2. Avoid Q5 or Q6 for 70B—they're ~45-50GB and offload more to CPU, slowing things down. Q4_0 is similar but slightly lower quality than Q4_K_M.\n",
       "  - **Performance Expectation**: 5-10 tokens/second on generation (faster for short responses). Full GPU usage for the loaded layers; the rest uses CPU. If you want faster speeds at the cost of capability, drop to Llama 3.1 8B (see below).\n",
       "- **How to Run It**:\n",
       "  1. Install Ollama (download from ollama.com).\n",
       "  2. Pull and run: `ollama run llama3.1:70b-instruct-q4_K_M` (this pulls a pre-quantized version; check Ollama's library for exact tags if needed, as they may vary).\n",
       "  3. Specify GPU layers if desired: Edit the Modelfile or use flags like `--gpu-layers 35` (experiment to max out your VRAM without OOM errors).\n",
       "  4. Test with a prompt: Something like \"Explain quantum computing in simple terms.\"\n",
       "\n",
       "If this feels too slow or you prioritize speed over raw capability (e.g., for chatbots or quick queries), go with a smaller model that fits fully in VRAM:\n",
       "\n",
       "### Faster Alternative: Llama 3.1 8B with Q8_0 Quantization\n",
       "- **Why?** This is still very capable (great for most everyday tasks) but runs blazing fast since it fits entirely in ~9GB VRAM. Speed: 40-60+ tokens/second.\n",
       "- **How to Run**: `ollama run llama3.1:8b-instruct-q8_0`.\n",
       "- **When to Choose This**: If 70B's speed is a dealbreaker, or for low-latency use cases.\n",
       "\n",
       "### Other Tips\n",
       "- **Upgrades for Better Performance**: If you can add more system RAM (e.g., 32GB+), it helps with offloading. For even larger models, consider external tools like llama.cpp for finer control.\n",
       "- **Model Library**: Browse `ollama list` or ollama.com/models for options. Variants like \"instruct\" are tuned for chat/following instructions.\n",
       "- **Troubleshooting**: If you get VRAM errors, reduce context size or layers. Update NVIDIA drivers for best CUDA support.\n",
       "- **Benchmarks**: Search Reddit (r/LocalLLaMA) or Hugging Face for user reports on RTX 3080 mobile with Ollama—experiences match what I've described.\n",
       "\n",
       "If you provide more details (e.g., your CPU/RAM, intended use case, or speed vs. quality preference), I can refine this further!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# xAI Grok - uses OpenAI-compatible API\n",
    "xai = OpenAI(api_key=xai_api_key, base_url=\"https://api.x.ai/v1\")\n",
    "model_name = \"grok-4\"\n",
    "\n",
    "response = xai.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For the next cell, we will use Ollama\n",
    "\n",
    "Ollama runs a local web service that gives an OpenAI compatible endpoint,  \n",
    "and runs models locally using high performance C++ code.\n",
    "\n",
    "If you don't have Ollama, install it here by visiting https://ollama.com then pressing Download and following the instructions.\n",
    "\n",
    "After it's installed, you should be able to visit here: http://localhost:11434 and see the message \"Ollama is running\"\n",
    "\n",
    "You might need to restart Cursor (and maybe reboot). Then open a Terminal (control+\\`) and run `ollama serve`\n",
    "\n",
    "Useful Ollama commands (run these in the terminal, or with an exclamation mark in this notebook):\n",
    "\n",
    "`ollama pull <model_name>` downloads a model locally  \n",
    "`ollama ls` lists all the models you've downloaded  \n",
    "`ollama rm <model_name>` deletes the specified model from your downloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/stop.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">Super important - ignore me at your peril!</h2>\n",
    "            <span style=\"color:#ff7800;\">The model called <b>llama3.3</b> is FAR too large for home computers - it's not intended for personal computing and will consume all your resources! Stick with the nicely sized <b>llama3.2</b> or <b>llama3.2:1b</b> and if you want larger, try llama3.1 or smaller variants of Qwen, Gemma, Phi or DeepSeek. See the <A href=\"https://ollama.com/models\">the Ollama models page</a> for a full list of models and sizes.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama pull qwen2.5:32b-instruct-q3_K_M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Running the largest model available while ensuring it works optimally within your GPU's 16GB VRAM limit requires some consideration of both model size and quantization techniques. The combination of a large model that fits in memory without causing performance degradation due to excessive swapping, along with efficient quantization, is key.\n",
       "\n",
       "For your RTX 3080 mobile GPU (with 16GB VRAM), several popular models could be considered:\n",
       "\n",
       "### Large Models\n",
       "- **llama2**: The LLaMA2 series includes models up to 70 billion parameters. However, the largest versions might exceed your VRAM capacity.\n",
       "- **Pythia**: Anthropic's larger Pythia models (like the 12B model) are known to work well.\n",
       "\n",
       "### Quantization\n",
       "Quantization reduces the precision of the weights in the model, which can significantly reduce the memory footprint. Common quantization formats include:\n",
       "- **4-bit**: Most efficient for memory but may compromise some performance.\n",
       "- **8-bit**: Less efficient than 4-bit but typically provides better performance and is often more compatible with existing models.\n",
       "\n",
       "### Suggestion\n",
       "Given your constraints (16GB VRAM), a good balance between model size and quantization could be the following:\n",
       "\n",
       "#### - **Pythia-12B with Quantization**:\n",
       "Using an 8-bit quantized version of Pythia-12B might work well. With 12 billion parameters, it should fit comfortably into your GPU's memory when properly quantized.\n",
       "\n",
       "Alternatively,\n",
       "\n",
       "#### - **LLaMA (or LLaMA2) Models under 65 Billion Parameters with Quantization**:\n",
       "An 8-bit or even a 4-bit quantized version of the smaller LLaMA2 models might also work well. You could start with an 8-bit quantized model and, if that does not fit as expected (considering overheads), try switching to a 4-bit version.\n",
       "\n",
       "### Testing\n",
       "It's essential to test these configurations on your laptop because various factors like other software background tasks can influence VRAM usage. The exact compatibility might vary based on the specific implementation of quantization used by your software stack.\n",
       "\n",
       "Always ensure that any model you use complies with the licensing and distribution policies set by its creators or distributors."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ollama = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "model_name = \"qwen2.5:32b-instruct-q3_K_M\"\n",
    "\n",
    "response = ollama.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['claude-sonnet-4-5-20250929', 'gemini-2.5-flash', 'grok-4', 'qwen2.5:32b-instruct-q3_K_M']\n",
      "['# Best Ollama Models for RTX 3080 Mobile (16GB VRAM)\\n\\nWith 16GB VRAM, you have several excellent options. Here are my recommendations:\\n\\n## Top Choices:\\n\\n### **1. Llama 3.1 70B Q2_K (Best Overall)**\\n```bash\\nollama run llama3.1:70b-instruct-q2_K\\n```\\n- **VRAM usage:** ~14-15GB\\n- **Quality:** Excellent reasoning despite Q2 quantization\\n- **Why:** The larger parameter count (70B) often outperforms smaller models even at lower quantization\\n\\n### **2. Qwen 2.5 32B Q4_K_M (Balanced)**\\n```bash\\nollama run qwen2.5:32b-instruct-q4_K_M\\n```\\n- **VRAM usage:** ~13-14GB\\n- **Quality:** Outstanding performance for coding and reasoning\\n- **Why:** Better quantization (Q4) with very capable 32B model\\n\\n### **3. Llama 3.1 70B Q3_K_M (If you want higher quality)**\\n```bash\\nollama run llama3.1:70b-instruct-q3_K_M\\n```\\n- **VRAM usage:** ~15.5-16GB (tight fit)\\n- **Quality:** Noticeably better than Q2\\n- **Why:** Pushes your VRAM limit but worth it\\n\\n### **4. Mistral Small 22B Q5_K_M (Speed + Quality)**\\n```bash\\nollama run mistral-small:22b-instruct-2409-q5_K_M\\n```\\n- **VRAM usage:** ~11-12GB\\n- **Quality:** Excellent quality with high quantization\\n- **Why:** Fast inference, great quality, headroom for other apps\\n\\n## My Recommendation:\\nStart with **Qwen 2.5 32B Q4_K_M** - it offers the best balance of quality, speed, and VRAM efficiency for most tasks, especially coding.\\n\\n## Quick Tips:\\n- Close other applications to maximize available VRAM\\n- Monitor temps on mobile GPU (they can throttle)\\n- Use `ollama ps` to check actual VRAM usage\\n\\nWhat will you primarily use the model for? I can refine the recommendation based on your use case.', 'That\\'s a fantastic setup for local AI! Your **RTX 3080 mobile with 16GB VRAM** is a sweet spot, allowing you to run very capable models entirely on your GPU.\\n\\nThe \"best\" model is always a balance of **quality, speed, and VRAM usage**. With 16GB, you have a good amount to play with.\\n\\nHere\\'s what I recommend, along with the ideal quantizations:\\n\\n---\\n\\n### Understanding Quantization (`Q_K_M`):\\n\\n*   **Quantization (Q)** reduces the precision of the model\\'s weights, making it smaller and faster, but with a potential slight loss in quality.\\n*   **`_K_M` (e.g., Q5_K_M)**: This is the modern, optimized quantization scheme for GGUF (the format Ollama uses). It offers a much better balance of speed and quality compared to older quantizations.\\n*   **Higher number = Less compression = Better quality / More VRAM / Slower.**\\n*   **Lower number = More compression = Lower quality / Less VRAM / Faster.**\\n\\n**General Recommendation for your 16GB VRAM:**\\nYou\\'ll generally want to aim for **Q5_K_M** or **Q6_K** for 7B/8B models, and **Q4_K_M** or **Q5_K_M** for 13B models.\\n\\n---\\n\\n### Top Model Recommendations for your 16GB VRAM:\\n\\n1.  **Llama 3 8B (Recommended - Best Overall Balance)**\\n    *   **Why:** Llama 3 is Meta\\'s latest and greatest. The 8B version is incredibly capable for its size, offering excellent reasoning, instruction following, and general knowledge. It\\'s often considered state-of-the-art for its parameter count.\\n    *   **Quantization:**\\n        *   **`llama3:8b-instruct-q6_K` (around 5.8GB VRAM)**: Excellent balance of quality and speed. This is probably the sweet spot.\\n        *   **`llama3:8b-instruct-q5_K_M` (around 5.1GB VRAM)**: Slightly smaller, very good quality, and slightly faster.\\n        *   **`llama3:8b-instruct-q8_0` (around 8.2GB VRAM)**: You can even run this for maximum quality on the 8B model, though the difference from q6_K is often minor.\\n    *   **How to run:** `ollama run llama3:8b-instruct-q6_K`\\n\\n2.  **Mistral 7B (Excellent Alternative - Fast & Creative)**\\n    *   **Why:** Mistral 7B is known for its speed, creativity, and good performance on a wide range of tasks. It\\'s a fantastic all-rounder and a bit faster than Llama 3 8B in some cases. Many popular fine-tunes are based on Mistral.\\n    *   **Quantization:**\\n        *   **`mistral:7b-instruct-v0.2-q6_K` (around 5.0GB VRAM)**: Great balance.\\n        *   **`mistral:7b-instruct-v0.2-q5_K_M` (around 4.4GB VRAM)**: Very fast and still high quality.\\n    *   **How to run:** `ollama run mistral:7b-instruct-v0.2-q6_K`\\n\\n3.  **OpenHermes 2.5 (Fine-tuned for Chat/Instruction Following)**\\n    *   **Why:** This is a fine-tune of Mistral 7B, specifically optimized for chat and instruction following. It\\'s often praised for its conversational abilities and willingness to follow complex prompts.\\n    *   **Quantization:**\\n        *   **`openhermes2.5-mistral:7b-q6_K` (around 5.0GB VRAM)**: Ideal for quality.\\n        *   **`openhermes2.5-mistral:7b-q5_K_M` (around 4.4GB VRAM)**: Still very good and zippy.\\n    *   **How to run:** `ollama run openhermes2.5-mistral:7b-q6_K`\\n\\n4.  **Llama 2 13B (For More \"Brainpower\" - Pushing VRAM)**\\n    *   **Why:** If you want a bit more raw capability than a 7B/8B model and don\\'t mind a slight speed reduction, a 13B model can offer more complex reasoning. While Llama 3 8B often beats Llama 2 13B, there are still scenarios where the larger parameter count can shine.\\n    *   **Quantization:**\\n        *   **`llama2:13b-chat-q5_K_M` (around 8.5GB VRAM)**: This will fit comfortably and offer good quality.\\n        *   **`llama2:13b-chat-q4_K_M` (around 7.5GB VRAM)**: A good option if you want to save a little VRAM or prefer slightly faster generation, with minimal perceived quality loss.\\n    *   **How to run:** `ollama run llama2:13b-chat-q5_K_M`\\n\\n---\\n\\n### What to AVOID (for full GPU offload):\\n\\n*   **Mixtral 8x7B (or any similar \"expert\" model):** While Mixtral is excellent, its \"8x7B\" architecture means it effectively uses weights equivalent to a much larger model (around 45-50B parameters) when all experts are considered. Even a Q4_K_M version of Mixtral is typically ~25-30GB, far exceeding your 16GB VRAM for full GPU offload. It will run very slowly with significant CPU offload.\\n\\n---\\n\\n### How to Get Started:\\n\\n1.  **Install Ollama:** If you haven\\'t already, download and install Ollama from [ollama.com](https://ollama.com/).\\n2.  **Download and Run:** Open your terminal or command prompt and use the `ollama run` command for the model and quantization you want. Ollama will automatically download it if you don\\'t have it.\\n    *   Example: `ollama run llama3:8b-instruct-q6_K`\\n3.  **Monitor VRAM:** While running, open `NVIDIA-smi` in another terminal window to monitor your GPU\\'s VRAM usage.\\n\\n### Pro Tip: Experiment!\\n\\nDownload a couple of these and try them out. You might find you prefer the \"personality\" or specific strengths of one over another for your particular use cases. Start with Llama 3 8B Q6_K, as it\\'s currently the go-to for many.\\n\\nEnjoy your powerful local AI setup!', 'Based on your setup (gaming laptop with RTX 3080 mobile and 16GB VRAM), I\\'ll recommend the best Ollama model in terms of capability (i.e., intelligence, reasoning, and output quality) that you can run effectively on your GPU. Ollama uses CUDA for NVIDIA GPU acceleration, and it supports automatic layer offloading—if the model doesn\\'t fully fit in VRAM, it loads as many layers as possible onto the GPU and falls back to CPU/RAM for the rest. This still gives a big performance boost over pure CPU.\\n\\n### Key Considerations for Your Hardware\\n- **VRAM Limits**: 16GB is solid, but large models (e.g., 70B+ parameters) won\\'t fit entirely in VRAM even when quantized (compressed). Expect ~10-40 layers on GPU depending on the model and quantization (with the rest on CPU). This can yield 5-15 tokens/second generation speed, which is usable for most tasks.\\n- **Quantization**: Lower quantization (e.g., Q4) reduces model size to fit more on GPU but slightly lowers quality. Higher quantization (e.g., Q8) preserves quality but requires more VRAM. I prioritize a balance of quality, size, and performance.\\n- **Other Factors**: Your laptop\\'s CPU (assuming something like an Intel i7 or Ryzen 7) and at least 16GB system RAM will help with offloading. Keep context size reasonable (e.g., 4K-8K tokens) to avoid VRAM overflow. Use Ollama\\'s latest version for optimal GPU support.\\n- **Testing**: Results can vary by exact laptop model (cooling, power limits). Start with `ollama run <model>` and monitor VRAM usage with tools like `nvidia-smi`.\\n\\n### Recommended Model: Llama 3.1 70B with Q4_K_M Quantization\\n- **Why this model?** Llama 3.1 70B is one of the most capable open-source models available in Ollama right now—excellent for complex reasoning, coding, writing, and general tasks. It\\'s significantly smarter than smaller models like 7B/8B or 13B variants, even if it runs slower due to partial offloading. For 16GB VRAM, this is the \"best\" (most advanced) you can realistically run without constant swapping or crashes. Alternatives like Mixtral 8x22B or Llama 3.1 405B are too big even with heavy quantization.\\n- **Quantization: Q4_K_M**\\n  - **Why Q4_K_M?** It\\'s a great balance: model size is ~40GB (fits ~25-35 layers on your 16GB GPU, depending on context). Quality is high (close to unquantized) with minimal loss compared to Q3 or Q2. Avoid Q5 or Q6 for 70B—they\\'re ~45-50GB and offload more to CPU, slowing things down. Q4_0 is similar but slightly lower quality than Q4_K_M.\\n  - **Performance Expectation**: 5-10 tokens/second on generation (faster for short responses). Full GPU usage for the loaded layers; the rest uses CPU. If you want faster speeds at the cost of capability, drop to Llama 3.1 8B (see below).\\n- **How to Run It**:\\n  1. Install Ollama (download from ollama.com).\\n  2. Pull and run: `ollama run llama3.1:70b-instruct-q4_K_M` (this pulls a pre-quantized version; check Ollama\\'s library for exact tags if needed, as they may vary).\\n  3. Specify GPU layers if desired: Edit the Modelfile or use flags like `--gpu-layers 35` (experiment to max out your VRAM without OOM errors).\\n  4. Test with a prompt: Something like \"Explain quantum computing in simple terms.\"\\n\\nIf this feels too slow or you prioritize speed over raw capability (e.g., for chatbots or quick queries), go with a smaller model that fits fully in VRAM:\\n\\n### Faster Alternative: Llama 3.1 8B with Q8_0 Quantization\\n- **Why?** This is still very capable (great for most everyday tasks) but runs blazing fast since it fits entirely in ~9GB VRAM. Speed: 40-60+ tokens/second.\\n- **How to Run**: `ollama run llama3.1:8b-instruct-q8_0`.\\n- **When to Choose This**: If 70B\\'s speed is a dealbreaker, or for low-latency use cases.\\n\\n### Other Tips\\n- **Upgrades for Better Performance**: If you can add more system RAM (e.g., 32GB+), it helps with offloading. For even larger models, consider external tools like llama.cpp for finer control.\\n- **Model Library**: Browse `ollama list` or ollama.com/models for options. Variants like \"instruct\" are tuned for chat/following instructions.\\n- **Troubleshooting**: If you get VRAM errors, reduce context size or layers. Update NVIDIA drivers for best CUDA support.\\n- **Benchmarks**: Search Reddit (r/LocalLLaMA) or Hugging Face for user reports on RTX 3080 mobile with Ollama—experiences match what I\\'ve described.\\n\\nIf you provide more details (e.g., your CPU/RAM, intended use case, or speed vs. quality preference), I can refine this further!', \"Running the largest model available while ensuring it works optimally within your GPU's 16GB VRAM limit requires some consideration of both model size and quantization techniques. The combination of a large model that fits in memory without causing performance degradation due to excessive swapping, along with efficient quantization, is key.\\n\\nFor your RTX 3080 mobile GPU (with 16GB VRAM), several popular models could be considered:\\n\\n### Large Models\\n- **llama2**: The LLaMA2 series includes models up to 70 billion parameters. However, the largest versions might exceed your VRAM capacity.\\n- **Pythia**: Anthropic's larger Pythia models (like the 12B model) are known to work well.\\n\\n### Quantization\\nQuantization reduces the precision of the weights in the model, which can significantly reduce the memory footprint. Common quantization formats include:\\n- **4-bit**: Most efficient for memory but may compromise some performance.\\n- **8-bit**: Less efficient than 4-bit but typically provides better performance and is often more compatible with existing models.\\n\\n### Suggestion\\nGiven your constraints (16GB VRAM), a good balance between model size and quantization could be the following:\\n\\n#### - **Pythia-12B with Quantization**:\\nUsing an 8-bit quantized version of Pythia-12B might work well. With 12 billion parameters, it should fit comfortably into your GPU's memory when properly quantized.\\n\\nAlternatively,\\n\\n#### - **LLaMA (or LLaMA2) Models under 65 Billion Parameters with Quantization**:\\nAn 8-bit or even a 4-bit quantized version of the smaller LLaMA2 models might also work well. You could start with an 8-bit quantized model and, if that does not fit as expected (considering overheads), try switching to a 4-bit version.\\n\\n### Testing\\nIt's essential to test these configurations on your laptop because various factors like other software background tasks can influence VRAM usage. The exact compatibility might vary based on the specific implementation of quantization used by your software stack.\\n\\nAlways ensure that any model you use complies with the licensing and distribution policies set by its creators or distributors.\"]\n"
     ]
    }
   ],
   "source": [
    "# So where are we?\n",
    "\n",
    "print(competitors)\n",
    "print(answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Competitor: claude-sonnet-4-5-20250929\n",
      "\n",
      "# Best Ollama Models for RTX 3080 Mobile (16GB VRAM)\n",
      "\n",
      "With 16GB VRAM, you have several excellent options. Here are my recommendations:\n",
      "\n",
      "## Top Choices:\n",
      "\n",
      "### **1. Llama 3.1 70B Q2_K (Best Overall)**\n",
      "```bash\n",
      "ollama run llama3.1:70b-instruct-q2_K\n",
      "```\n",
      "- **VRAM usage:** ~14-15GB\n",
      "- **Quality:** Excellent reasoning despite Q2 quantization\n",
      "- **Why:** The larger parameter count (70B) often outperforms smaller models even at lower quantization\n",
      "\n",
      "### **2. Qwen 2.5 32B Q4_K_M (Balanced)**\n",
      "```bash\n",
      "ollama run qwen2.5:32b-instruct-q4_K_M\n",
      "```\n",
      "- **VRAM usage:** ~13-14GB\n",
      "- **Quality:** Outstanding performance for coding and reasoning\n",
      "- **Why:** Better quantization (Q4) with very capable 32B model\n",
      "\n",
      "### **3. Llama 3.1 70B Q3_K_M (If you want higher quality)**\n",
      "```bash\n",
      "ollama run llama3.1:70b-instruct-q3_K_M\n",
      "```\n",
      "- **VRAM usage:** ~15.5-16GB (tight fit)\n",
      "- **Quality:** Noticeably better than Q2\n",
      "- **Why:** Pushes your VRAM limit but worth it\n",
      "\n",
      "### **4. Mistral Small 22B Q5_K_M (Speed + Quality)**\n",
      "```bash\n",
      "ollama run mistral-small:22b-instruct-2409-q5_K_M\n",
      "```\n",
      "- **VRAM usage:** ~11-12GB\n",
      "- **Quality:** Excellent quality with high quantization\n",
      "- **Why:** Fast inference, great quality, headroom for other apps\n",
      "\n",
      "## My Recommendation:\n",
      "Start with **Qwen 2.5 32B Q4_K_M** - it offers the best balance of quality, speed, and VRAM efficiency for most tasks, especially coding.\n",
      "\n",
      "## Quick Tips:\n",
      "- Close other applications to maximize available VRAM\n",
      "- Monitor temps on mobile GPU (they can throttle)\n",
      "- Use `ollama ps` to check actual VRAM usage\n",
      "\n",
      "What will you primarily use the model for? I can refine the recommendation based on your use case.\n",
      "Competitor: gemini-2.5-flash\n",
      "\n",
      "That's a fantastic setup for local AI! Your **RTX 3080 mobile with 16GB VRAM** is a sweet spot, allowing you to run very capable models entirely on your GPU.\n",
      "\n",
      "The \"best\" model is always a balance of **quality, speed, and VRAM usage**. With 16GB, you have a good amount to play with.\n",
      "\n",
      "Here's what I recommend, along with the ideal quantizations:\n",
      "\n",
      "---\n",
      "\n",
      "### Understanding Quantization (`Q_K_M`):\n",
      "\n",
      "*   **Quantization (Q)** reduces the precision of the model's weights, making it smaller and faster, but with a potential slight loss in quality.\n",
      "*   **`_K_M` (e.g., Q5_K_M)**: This is the modern, optimized quantization scheme for GGUF (the format Ollama uses). It offers a much better balance of speed and quality compared to older quantizations.\n",
      "*   **Higher number = Less compression = Better quality / More VRAM / Slower.**\n",
      "*   **Lower number = More compression = Lower quality / Less VRAM / Faster.**\n",
      "\n",
      "**General Recommendation for your 16GB VRAM:**\n",
      "You'll generally want to aim for **Q5_K_M** or **Q6_K** for 7B/8B models, and **Q4_K_M** or **Q5_K_M** for 13B models.\n",
      "\n",
      "---\n",
      "\n",
      "### Top Model Recommendations for your 16GB VRAM:\n",
      "\n",
      "1.  **Llama 3 8B (Recommended - Best Overall Balance)**\n",
      "    *   **Why:** Llama 3 is Meta's latest and greatest. The 8B version is incredibly capable for its size, offering excellent reasoning, instruction following, and general knowledge. It's often considered state-of-the-art for its parameter count.\n",
      "    *   **Quantization:**\n",
      "        *   **`llama3:8b-instruct-q6_K` (around 5.8GB VRAM)**: Excellent balance of quality and speed. This is probably the sweet spot.\n",
      "        *   **`llama3:8b-instruct-q5_K_M` (around 5.1GB VRAM)**: Slightly smaller, very good quality, and slightly faster.\n",
      "        *   **`llama3:8b-instruct-q8_0` (around 8.2GB VRAM)**: You can even run this for maximum quality on the 8B model, though the difference from q6_K is often minor.\n",
      "    *   **How to run:** `ollama run llama3:8b-instruct-q6_K`\n",
      "\n",
      "2.  **Mistral 7B (Excellent Alternative - Fast & Creative)**\n",
      "    *   **Why:** Mistral 7B is known for its speed, creativity, and good performance on a wide range of tasks. It's a fantastic all-rounder and a bit faster than Llama 3 8B in some cases. Many popular fine-tunes are based on Mistral.\n",
      "    *   **Quantization:**\n",
      "        *   **`mistral:7b-instruct-v0.2-q6_K` (around 5.0GB VRAM)**: Great balance.\n",
      "        *   **`mistral:7b-instruct-v0.2-q5_K_M` (around 4.4GB VRAM)**: Very fast and still high quality.\n",
      "    *   **How to run:** `ollama run mistral:7b-instruct-v0.2-q6_K`\n",
      "\n",
      "3.  **OpenHermes 2.5 (Fine-tuned for Chat/Instruction Following)**\n",
      "    *   **Why:** This is a fine-tune of Mistral 7B, specifically optimized for chat and instruction following. It's often praised for its conversational abilities and willingness to follow complex prompts.\n",
      "    *   **Quantization:**\n",
      "        *   **`openhermes2.5-mistral:7b-q6_K` (around 5.0GB VRAM)**: Ideal for quality.\n",
      "        *   **`openhermes2.5-mistral:7b-q5_K_M` (around 4.4GB VRAM)**: Still very good and zippy.\n",
      "    *   **How to run:** `ollama run openhermes2.5-mistral:7b-q6_K`\n",
      "\n",
      "4.  **Llama 2 13B (For More \"Brainpower\" - Pushing VRAM)**\n",
      "    *   **Why:** If you want a bit more raw capability than a 7B/8B model and don't mind a slight speed reduction, a 13B model can offer more complex reasoning. While Llama 3 8B often beats Llama 2 13B, there are still scenarios where the larger parameter count can shine.\n",
      "    *   **Quantization:**\n",
      "        *   **`llama2:13b-chat-q5_K_M` (around 8.5GB VRAM)**: This will fit comfortably and offer good quality.\n",
      "        *   **`llama2:13b-chat-q4_K_M` (around 7.5GB VRAM)**: A good option if you want to save a little VRAM or prefer slightly faster generation, with minimal perceived quality loss.\n",
      "    *   **How to run:** `ollama run llama2:13b-chat-q5_K_M`\n",
      "\n",
      "---\n",
      "\n",
      "### What to AVOID (for full GPU offload):\n",
      "\n",
      "*   **Mixtral 8x7B (or any similar \"expert\" model):** While Mixtral is excellent, its \"8x7B\" architecture means it effectively uses weights equivalent to a much larger model (around 45-50B parameters) when all experts are considered. Even a Q4_K_M version of Mixtral is typically ~25-30GB, far exceeding your 16GB VRAM for full GPU offload. It will run very slowly with significant CPU offload.\n",
      "\n",
      "---\n",
      "\n",
      "### How to Get Started:\n",
      "\n",
      "1.  **Install Ollama:** If you haven't already, download and install Ollama from [ollama.com](https://ollama.com/).\n",
      "2.  **Download and Run:** Open your terminal or command prompt and use the `ollama run` command for the model and quantization you want. Ollama will automatically download it if you don't have it.\n",
      "    *   Example: `ollama run llama3:8b-instruct-q6_K`\n",
      "3.  **Monitor VRAM:** While running, open `NVIDIA-smi` in another terminal window to monitor your GPU's VRAM usage.\n",
      "\n",
      "### Pro Tip: Experiment!\n",
      "\n",
      "Download a couple of these and try them out. You might find you prefer the \"personality\" or specific strengths of one over another for your particular use cases. Start with Llama 3 8B Q6_K, as it's currently the go-to for many.\n",
      "\n",
      "Enjoy your powerful local AI setup!\n",
      "Competitor: grok-4\n",
      "\n",
      "Based on your setup (gaming laptop with RTX 3080 mobile and 16GB VRAM), I'll recommend the best Ollama model in terms of capability (i.e., intelligence, reasoning, and output quality) that you can run effectively on your GPU. Ollama uses CUDA for NVIDIA GPU acceleration, and it supports automatic layer offloading—if the model doesn't fully fit in VRAM, it loads as many layers as possible onto the GPU and falls back to CPU/RAM for the rest. This still gives a big performance boost over pure CPU.\n",
      "\n",
      "### Key Considerations for Your Hardware\n",
      "- **VRAM Limits**: 16GB is solid, but large models (e.g., 70B+ parameters) won't fit entirely in VRAM even when quantized (compressed). Expect ~10-40 layers on GPU depending on the model and quantization (with the rest on CPU). This can yield 5-15 tokens/second generation speed, which is usable for most tasks.\n",
      "- **Quantization**: Lower quantization (e.g., Q4) reduces model size to fit more on GPU but slightly lowers quality. Higher quantization (e.g., Q8) preserves quality but requires more VRAM. I prioritize a balance of quality, size, and performance.\n",
      "- **Other Factors**: Your laptop's CPU (assuming something like an Intel i7 or Ryzen 7) and at least 16GB system RAM will help with offloading. Keep context size reasonable (e.g., 4K-8K tokens) to avoid VRAM overflow. Use Ollama's latest version for optimal GPU support.\n",
      "- **Testing**: Results can vary by exact laptop model (cooling, power limits). Start with `ollama run <model>` and monitor VRAM usage with tools like `nvidia-smi`.\n",
      "\n",
      "### Recommended Model: Llama 3.1 70B with Q4_K_M Quantization\n",
      "- **Why this model?** Llama 3.1 70B is one of the most capable open-source models available in Ollama right now—excellent for complex reasoning, coding, writing, and general tasks. It's significantly smarter than smaller models like 7B/8B or 13B variants, even if it runs slower due to partial offloading. For 16GB VRAM, this is the \"best\" (most advanced) you can realistically run without constant swapping or crashes. Alternatives like Mixtral 8x22B or Llama 3.1 405B are too big even with heavy quantization.\n",
      "- **Quantization: Q4_K_M**\n",
      "  - **Why Q4_K_M?** It's a great balance: model size is ~40GB (fits ~25-35 layers on your 16GB GPU, depending on context). Quality is high (close to unquantized) with minimal loss compared to Q3 or Q2. Avoid Q5 or Q6 for 70B—they're ~45-50GB and offload more to CPU, slowing things down. Q4_0 is similar but slightly lower quality than Q4_K_M.\n",
      "  - **Performance Expectation**: 5-10 tokens/second on generation (faster for short responses). Full GPU usage for the loaded layers; the rest uses CPU. If you want faster speeds at the cost of capability, drop to Llama 3.1 8B (see below).\n",
      "- **How to Run It**:\n",
      "  1. Install Ollama (download from ollama.com).\n",
      "  2. Pull and run: `ollama run llama3.1:70b-instruct-q4_K_M` (this pulls a pre-quantized version; check Ollama's library for exact tags if needed, as they may vary).\n",
      "  3. Specify GPU layers if desired: Edit the Modelfile or use flags like `--gpu-layers 35` (experiment to max out your VRAM without OOM errors).\n",
      "  4. Test with a prompt: Something like \"Explain quantum computing in simple terms.\"\n",
      "\n",
      "If this feels too slow or you prioritize speed over raw capability (e.g., for chatbots or quick queries), go with a smaller model that fits fully in VRAM:\n",
      "\n",
      "### Faster Alternative: Llama 3.1 8B with Q8_0 Quantization\n",
      "- **Why?** This is still very capable (great for most everyday tasks) but runs blazing fast since it fits entirely in ~9GB VRAM. Speed: 40-60+ tokens/second.\n",
      "- **How to Run**: `ollama run llama3.1:8b-instruct-q8_0`.\n",
      "- **When to Choose This**: If 70B's speed is a dealbreaker, or for low-latency use cases.\n",
      "\n",
      "### Other Tips\n",
      "- **Upgrades for Better Performance**: If you can add more system RAM (e.g., 32GB+), it helps with offloading. For even larger models, consider external tools like llama.cpp for finer control.\n",
      "- **Model Library**: Browse `ollama list` or ollama.com/models for options. Variants like \"instruct\" are tuned for chat/following instructions.\n",
      "- **Troubleshooting**: If you get VRAM errors, reduce context size or layers. Update NVIDIA drivers for best CUDA support.\n",
      "- **Benchmarks**: Search Reddit (r/LocalLLaMA) or Hugging Face for user reports on RTX 3080 mobile with Ollama—experiences match what I've described.\n",
      "\n",
      "If you provide more details (e.g., your CPU/RAM, intended use case, or speed vs. quality preference), I can refine this further!\n",
      "Competitor: qwen2.5:32b-instruct-q3_K_M\n",
      "\n",
      "Running the largest model available while ensuring it works optimally within your GPU's 16GB VRAM limit requires some consideration of both model size and quantization techniques. The combination of a large model that fits in memory without causing performance degradation due to excessive swapping, along with efficient quantization, is key.\n",
      "\n",
      "For your RTX 3080 mobile GPU (with 16GB VRAM), several popular models could be considered:\n",
      "\n",
      "### Large Models\n",
      "- **llama2**: The LLaMA2 series includes models up to 70 billion parameters. However, the largest versions might exceed your VRAM capacity.\n",
      "- **Pythia**: Anthropic's larger Pythia models (like the 12B model) are known to work well.\n",
      "\n",
      "### Quantization\n",
      "Quantization reduces the precision of the weights in the model, which can significantly reduce the memory footprint. Common quantization formats include:\n",
      "- **4-bit**: Most efficient for memory but may compromise some performance.\n",
      "- **8-bit**: Less efficient than 4-bit but typically provides better performance and is often more compatible with existing models.\n",
      "\n",
      "### Suggestion\n",
      "Given your constraints (16GB VRAM), a good balance between model size and quantization could be the following:\n",
      "\n",
      "#### - **Pythia-12B with Quantization**:\n",
      "Using an 8-bit quantized version of Pythia-12B might work well. With 12 billion parameters, it should fit comfortably into your GPU's memory when properly quantized.\n",
      "\n",
      "Alternatively,\n",
      "\n",
      "#### - **LLaMA (or LLaMA2) Models under 65 Billion Parameters with Quantization**:\n",
      "An 8-bit or even a 4-bit quantized version of the smaller LLaMA2 models might also work well. You could start with an 8-bit quantized model and, if that does not fit as expected (considering overheads), try switching to a 4-bit version.\n",
      "\n",
      "### Testing\n",
      "It's essential to test these configurations on your laptop because various factors like other software background tasks can influence VRAM usage. The exact compatibility might vary based on the specific implementation of quantization used by your software stack.\n",
      "\n",
      "Always ensure that any model you use complies with the licensing and distribution policies set by its creators or distributors.\n"
     ]
    }
   ],
   "source": [
    "# It's nice to know how to use \"zip\"\n",
    "for competitor, answer in zip(competitors, answers):\n",
    "    print(f\"Competitor: {competitor}\\n\\n{answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's bring this together - note the use of \"enumerate\"\n",
    "\n",
    "together = \"\"\n",
    "for index, answer in enumerate(answers):\n",
    "    together += f\"# Response from competitor {index+1}\\n\\n\"\n",
    "    together += answer + \"\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Response from competitor 1\n",
      "\n",
      "# Best Ollama Models for RTX 3080 Mobile (16GB VRAM)\n",
      "\n",
      "With 16GB VRAM, you have several excellent options. Here are my recommendations:\n",
      "\n",
      "## Top Choices:\n",
      "\n",
      "### **1. Llama 3.1 70B Q2_K (Best Overall)**\n",
      "```bash\n",
      "ollama run llama3.1:70b-instruct-q2_K\n",
      "```\n",
      "- **VRAM usage:** ~14-15GB\n",
      "- **Quality:** Excellent reasoning despite Q2 quantization\n",
      "- **Why:** The larger parameter count (70B) often outperforms smaller models even at lower quantization\n",
      "\n",
      "### **2. Qwen 2.5 32B Q4_K_M (Balanced)**\n",
      "```bash\n",
      "ollama run qwen2.5:32b-instruct-q4_K_M\n",
      "```\n",
      "- **VRAM usage:** ~13-14GB\n",
      "- **Quality:** Outstanding performance for coding and reasoning\n",
      "- **Why:** Better quantization (Q4) with very capable 32B model\n",
      "\n",
      "### **3. Llama 3.1 70B Q3_K_M (If you want higher quality)**\n",
      "```bash\n",
      "ollama run llama3.1:70b-instruct-q3_K_M\n",
      "```\n",
      "- **VRAM usage:** ~15.5-16GB (tight fit)\n",
      "- **Quality:** Noticeably better than Q2\n",
      "- **Why:** Pushes your VRAM limit but worth it\n",
      "\n",
      "### **4. Mistral Small 22B Q5_K_M (Speed + Quality)**\n",
      "```bash\n",
      "ollama run mistral-small:22b-instruct-2409-q5_K_M\n",
      "```\n",
      "- **VRAM usage:** ~11-12GB\n",
      "- **Quality:** Excellent quality with high quantization\n",
      "- **Why:** Fast inference, great quality, headroom for other apps\n",
      "\n",
      "## My Recommendation:\n",
      "Start with **Qwen 2.5 32B Q4_K_M** - it offers the best balance of quality, speed, and VRAM efficiency for most tasks, especially coding.\n",
      "\n",
      "## Quick Tips:\n",
      "- Close other applications to maximize available VRAM\n",
      "- Monitor temps on mobile GPU (they can throttle)\n",
      "- Use `ollama ps` to check actual VRAM usage\n",
      "\n",
      "What will you primarily use the model for? I can refine the recommendation based on your use case.\n",
      "\n",
      "# Response from competitor 2\n",
      "\n",
      "That's a fantastic setup for local AI! Your **RTX 3080 mobile with 16GB VRAM** is a sweet spot, allowing you to run very capable models entirely on your GPU.\n",
      "\n",
      "The \"best\" model is always a balance of **quality, speed, and VRAM usage**. With 16GB, you have a good amount to play with.\n",
      "\n",
      "Here's what I recommend, along with the ideal quantizations:\n",
      "\n",
      "---\n",
      "\n",
      "### Understanding Quantization (`Q_K_M`):\n",
      "\n",
      "*   **Quantization (Q)** reduces the precision of the model's weights, making it smaller and faster, but with a potential slight loss in quality.\n",
      "*   **`_K_M` (e.g., Q5_K_M)**: This is the modern, optimized quantization scheme for GGUF (the format Ollama uses). It offers a much better balance of speed and quality compared to older quantizations.\n",
      "*   **Higher number = Less compression = Better quality / More VRAM / Slower.**\n",
      "*   **Lower number = More compression = Lower quality / Less VRAM / Faster.**\n",
      "\n",
      "**General Recommendation for your 16GB VRAM:**\n",
      "You'll generally want to aim for **Q5_K_M** or **Q6_K** for 7B/8B models, and **Q4_K_M** or **Q5_K_M** for 13B models.\n",
      "\n",
      "---\n",
      "\n",
      "### Top Model Recommendations for your 16GB VRAM:\n",
      "\n",
      "1.  **Llama 3 8B (Recommended - Best Overall Balance)**\n",
      "    *   **Why:** Llama 3 is Meta's latest and greatest. The 8B version is incredibly capable for its size, offering excellent reasoning, instruction following, and general knowledge. It's often considered state-of-the-art for its parameter count.\n",
      "    *   **Quantization:**\n",
      "        *   **`llama3:8b-instruct-q6_K` (around 5.8GB VRAM)**: Excellent balance of quality and speed. This is probably the sweet spot.\n",
      "        *   **`llama3:8b-instruct-q5_K_M` (around 5.1GB VRAM)**: Slightly smaller, very good quality, and slightly faster.\n",
      "        *   **`llama3:8b-instruct-q8_0` (around 8.2GB VRAM)**: You can even run this for maximum quality on the 8B model, though the difference from q6_K is often minor.\n",
      "    *   **How to run:** `ollama run llama3:8b-instruct-q6_K`\n",
      "\n",
      "2.  **Mistral 7B (Excellent Alternative - Fast & Creative)**\n",
      "    *   **Why:** Mistral 7B is known for its speed, creativity, and good performance on a wide range of tasks. It's a fantastic all-rounder and a bit faster than Llama 3 8B in some cases. Many popular fine-tunes are based on Mistral.\n",
      "    *   **Quantization:**\n",
      "        *   **`mistral:7b-instruct-v0.2-q6_K` (around 5.0GB VRAM)**: Great balance.\n",
      "        *   **`mistral:7b-instruct-v0.2-q5_K_M` (around 4.4GB VRAM)**: Very fast and still high quality.\n",
      "    *   **How to run:** `ollama run mistral:7b-instruct-v0.2-q6_K`\n",
      "\n",
      "3.  **OpenHermes 2.5 (Fine-tuned for Chat/Instruction Following)**\n",
      "    *   **Why:** This is a fine-tune of Mistral 7B, specifically optimized for chat and instruction following. It's often praised for its conversational abilities and willingness to follow complex prompts.\n",
      "    *   **Quantization:**\n",
      "        *   **`openhermes2.5-mistral:7b-q6_K` (around 5.0GB VRAM)**: Ideal for quality.\n",
      "        *   **`openhermes2.5-mistral:7b-q5_K_M` (around 4.4GB VRAM)**: Still very good and zippy.\n",
      "    *   **How to run:** `ollama run openhermes2.5-mistral:7b-q6_K`\n",
      "\n",
      "4.  **Llama 2 13B (For More \"Brainpower\" - Pushing VRAM)**\n",
      "    *   **Why:** If you want a bit more raw capability than a 7B/8B model and don't mind a slight speed reduction, a 13B model can offer more complex reasoning. While Llama 3 8B often beats Llama 2 13B, there are still scenarios where the larger parameter count can shine.\n",
      "    *   **Quantization:**\n",
      "        *   **`llama2:13b-chat-q5_K_M` (around 8.5GB VRAM)**: This will fit comfortably and offer good quality.\n",
      "        *   **`llama2:13b-chat-q4_K_M` (around 7.5GB VRAM)**: A good option if you want to save a little VRAM or prefer slightly faster generation, with minimal perceived quality loss.\n",
      "    *   **How to run:** `ollama run llama2:13b-chat-q5_K_M`\n",
      "\n",
      "---\n",
      "\n",
      "### What to AVOID (for full GPU offload):\n",
      "\n",
      "*   **Mixtral 8x7B (or any similar \"expert\" model):** While Mixtral is excellent, its \"8x7B\" architecture means it effectively uses weights equivalent to a much larger model (around 45-50B parameters) when all experts are considered. Even a Q4_K_M version of Mixtral is typically ~25-30GB, far exceeding your 16GB VRAM for full GPU offload. It will run very slowly with significant CPU offload.\n",
      "\n",
      "---\n",
      "\n",
      "### How to Get Started:\n",
      "\n",
      "1.  **Install Ollama:** If you haven't already, download and install Ollama from [ollama.com](https://ollama.com/).\n",
      "2.  **Download and Run:** Open your terminal or command prompt and use the `ollama run` command for the model and quantization you want. Ollama will automatically download it if you don't have it.\n",
      "    *   Example: `ollama run llama3:8b-instruct-q6_K`\n",
      "3.  **Monitor VRAM:** While running, open `NVIDIA-smi` in another terminal window to monitor your GPU's VRAM usage.\n",
      "\n",
      "### Pro Tip: Experiment!\n",
      "\n",
      "Download a couple of these and try them out. You might find you prefer the \"personality\" or specific strengths of one over another for your particular use cases. Start with Llama 3 8B Q6_K, as it's currently the go-to for many.\n",
      "\n",
      "Enjoy your powerful local AI setup!\n",
      "\n",
      "# Response from competitor 3\n",
      "\n",
      "Based on your setup (gaming laptop with RTX 3080 mobile and 16GB VRAM), I'll recommend the best Ollama model in terms of capability (i.e., intelligence, reasoning, and output quality) that you can run effectively on your GPU. Ollama uses CUDA for NVIDIA GPU acceleration, and it supports automatic layer offloading—if the model doesn't fully fit in VRAM, it loads as many layers as possible onto the GPU and falls back to CPU/RAM for the rest. This still gives a big performance boost over pure CPU.\n",
      "\n",
      "### Key Considerations for Your Hardware\n",
      "- **VRAM Limits**: 16GB is solid, but large models (e.g., 70B+ parameters) won't fit entirely in VRAM even when quantized (compressed). Expect ~10-40 layers on GPU depending on the model and quantization (with the rest on CPU). This can yield 5-15 tokens/second generation speed, which is usable for most tasks.\n",
      "- **Quantization**: Lower quantization (e.g., Q4) reduces model size to fit more on GPU but slightly lowers quality. Higher quantization (e.g., Q8) preserves quality but requires more VRAM. I prioritize a balance of quality, size, and performance.\n",
      "- **Other Factors**: Your laptop's CPU (assuming something like an Intel i7 or Ryzen 7) and at least 16GB system RAM will help with offloading. Keep context size reasonable (e.g., 4K-8K tokens) to avoid VRAM overflow. Use Ollama's latest version for optimal GPU support.\n",
      "- **Testing**: Results can vary by exact laptop model (cooling, power limits). Start with `ollama run <model>` and monitor VRAM usage with tools like `nvidia-smi`.\n",
      "\n",
      "### Recommended Model: Llama 3.1 70B with Q4_K_M Quantization\n",
      "- **Why this model?** Llama 3.1 70B is one of the most capable open-source models available in Ollama right now—excellent for complex reasoning, coding, writing, and general tasks. It's significantly smarter than smaller models like 7B/8B or 13B variants, even if it runs slower due to partial offloading. For 16GB VRAM, this is the \"best\" (most advanced) you can realistically run without constant swapping or crashes. Alternatives like Mixtral 8x22B or Llama 3.1 405B are too big even with heavy quantization.\n",
      "- **Quantization: Q4_K_M**\n",
      "  - **Why Q4_K_M?** It's a great balance: model size is ~40GB (fits ~25-35 layers on your 16GB GPU, depending on context). Quality is high (close to unquantized) with minimal loss compared to Q3 or Q2. Avoid Q5 or Q6 for 70B—they're ~45-50GB and offload more to CPU, slowing things down. Q4_0 is similar but slightly lower quality than Q4_K_M.\n",
      "  - **Performance Expectation**: 5-10 tokens/second on generation (faster for short responses). Full GPU usage for the loaded layers; the rest uses CPU. If you want faster speeds at the cost of capability, drop to Llama 3.1 8B (see below).\n",
      "- **How to Run It**:\n",
      "  1. Install Ollama (download from ollama.com).\n",
      "  2. Pull and run: `ollama run llama3.1:70b-instruct-q4_K_M` (this pulls a pre-quantized version; check Ollama's library for exact tags if needed, as they may vary).\n",
      "  3. Specify GPU layers if desired: Edit the Modelfile or use flags like `--gpu-layers 35` (experiment to max out your VRAM without OOM errors).\n",
      "  4. Test with a prompt: Something like \"Explain quantum computing in simple terms.\"\n",
      "\n",
      "If this feels too slow or you prioritize speed over raw capability (e.g., for chatbots or quick queries), go with a smaller model that fits fully in VRAM:\n",
      "\n",
      "### Faster Alternative: Llama 3.1 8B with Q8_0 Quantization\n",
      "- **Why?** This is still very capable (great for most everyday tasks) but runs blazing fast since it fits entirely in ~9GB VRAM. Speed: 40-60+ tokens/second.\n",
      "- **How to Run**: `ollama run llama3.1:8b-instruct-q8_0`.\n",
      "- **When to Choose This**: If 70B's speed is a dealbreaker, or for low-latency use cases.\n",
      "\n",
      "### Other Tips\n",
      "- **Upgrades for Better Performance**: If you can add more system RAM (e.g., 32GB+), it helps with offloading. For even larger models, consider external tools like llama.cpp for finer control.\n",
      "- **Model Library**: Browse `ollama list` or ollama.com/models for options. Variants like \"instruct\" are tuned for chat/following instructions.\n",
      "- **Troubleshooting**: If you get VRAM errors, reduce context size or layers. Update NVIDIA drivers for best CUDA support.\n",
      "- **Benchmarks**: Search Reddit (r/LocalLLaMA) or Hugging Face for user reports on RTX 3080 mobile with Ollama—experiences match what I've described.\n",
      "\n",
      "If you provide more details (e.g., your CPU/RAM, intended use case, or speed vs. quality preference), I can refine this further!\n",
      "\n",
      "# Response from competitor 4\n",
      "\n",
      "Running the largest model available while ensuring it works optimally within your GPU's 16GB VRAM limit requires some consideration of both model size and quantization techniques. The combination of a large model that fits in memory without causing performance degradation due to excessive swapping, along with efficient quantization, is key.\n",
      "\n",
      "For your RTX 3080 mobile GPU (with 16GB VRAM), several popular models could be considered:\n",
      "\n",
      "### Large Models\n",
      "- **llama2**: The LLaMA2 series includes models up to 70 billion parameters. However, the largest versions might exceed your VRAM capacity.\n",
      "- **Pythia**: Anthropic's larger Pythia models (like the 12B model) are known to work well.\n",
      "\n",
      "### Quantization\n",
      "Quantization reduces the precision of the weights in the model, which can significantly reduce the memory footprint. Common quantization formats include:\n",
      "- **4-bit**: Most efficient for memory but may compromise some performance.\n",
      "- **8-bit**: Less efficient than 4-bit but typically provides better performance and is often more compatible with existing models.\n",
      "\n",
      "### Suggestion\n",
      "Given your constraints (16GB VRAM), a good balance between model size and quantization could be the following:\n",
      "\n",
      "#### - **Pythia-12B with Quantization**:\n",
      "Using an 8-bit quantized version of Pythia-12B might work well. With 12 billion parameters, it should fit comfortably into your GPU's memory when properly quantized.\n",
      "\n",
      "Alternatively,\n",
      "\n",
      "#### - **LLaMA (or LLaMA2) Models under 65 Billion Parameters with Quantization**:\n",
      "An 8-bit or even a 4-bit quantized version of the smaller LLaMA2 models might also work well. You could start with an 8-bit quantized model and, if that does not fit as expected (considering overheads), try switching to a 4-bit version.\n",
      "\n",
      "### Testing\n",
      "It's essential to test these configurations on your laptop because various factors like other software background tasks can influence VRAM usage. The exact compatibility might vary based on the specific implementation of quantization used by your software stack.\n",
      "\n",
      "Always ensure that any model you use complies with the licensing and distribution policies set by its creators or distributors.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(together)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge = f\"\"\"You are judging a competition between {len(competitors)} competitors.\n",
    "Each model has been given this question:\n",
    "\n",
    "{question}\n",
    "\n",
    "Your job is to evaluate each response for clarity and strength of argument, and rank them in order of best to worst.\n",
    "Respond with JSON, and only JSON, with the following format:\n",
    "{{\"results\": [\"best competitor number\", \"second best competitor number\", \"third best competitor number\", ...]}}\n",
    "\n",
    "Here are the responses from each competitor:\n",
    "\n",
    "{together}\n",
    "\n",
    "Now respond with the JSON with the ranked order of the competitors, nothing else. Do not include markdown formatting or code blocks.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge = f\"\"\"You are judging a competition between {len(competitors)} competitors.\n",
    "Each model has been given this question:\n",
    "\n",
    "{question}\n",
    "\n",
    "Your job is to evaluate each response for clarity and strength of argument, and find the top 3 models cummulated from all responses and why you renk them so high.\n",
    "add also the model qwen2.5:32b-instruct-q3_K_M into you considerations.\n",
    "Here are the responses from each competitor:\n",
    "\n",
    "{together}\n",
    "\n",
    "Now respond with the JSON with the ranked order of the competitors.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'judge' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mjudge\u001b[49m)\n",
      "\u001b[31mNameError\u001b[39m: name 'judge' is not defined"
     ]
    }
   ],
   "source": [
    "print(judge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_messages = [{\"role\": \"user\", \"content\": judge}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Judgement time!\n",
    "\n",
    "# openai = OpenAI()\n",
    "# response = openai.chat.completions.create(\n",
    "#     model=\"o3-mini\",\n",
    "#     messages=judge_messages,\n",
    "# )\n",
    "# results = response.choices[0].message.content\n",
    "# print(results)\n",
    "\n",
    "model_name = \"claude-sonnet-4-5-20250929\"\n",
    "\n",
    "claude = Anthropic()\n",
    "response = claude.messages.create(model=model_name, messages=judge_messages, max_tokens=4000)\n",
    "results = response.content[0].text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"ranking\": [\n",
      "    {\n",
      "      \"rank\": 1,\n",
      "      \"competitor\": 1,\n",
      "      \"justification\": \"Competitor 1 provides the most comprehensive and practical response. They offer four well-reasoned options with specific commands, VRAM usage estimates, and clear trade-offs. The response directly addresses multiple use cases and includes the most relevant models for 16GB VRAM. The recommendation of Qwen 2.5 32B Q4_K_M as the starting point is excellent, and they also intelligently suggest Llama 3.1 70B at Q2_K and Q3_K_M quantizations - pushing the boundaries of what's possible with 16GB VRAM while maintaining usability.\"\n",
      "    },\n",
      "    {\n",
      "      \"rank\": 2,\n",
      "      \"competitor\": 3,\n",
      "      \"justification\": \"Competitor 3 delivers a technically detailed and well-reasoned response. They provide excellent context about layer offloading, realistic performance expectations (5-10 tokens/second), and practical considerations for laptop usage. The recommendation of Llama 3.1 70B Q4_K_M is solid, prioritizing capability over speed. They also provide a good alternative with Llama 3.1 8B Q8_0 for users prioritizing speed. The response includes helpful troubleshooting tips and demonstrates deep understanding of the hardware constraints.\"\n",
      "    },\n",
      "    {\n",
      "      \"rank\": 3,\n",
      "      \"competitor\": 2,\n",
      "      \"justification\": \"Competitor 2 provides a well-structured, beginner-friendly response with good explanations of quantization concepts. However, they focus primarily on smaller 7B-8B and 13B models, missing the opportunity to recommend larger models (like 32B or 70B variants) that could actually run on 16GB VRAM with appropriate quantization. While their recommendations are safe and will work well, they don't push the hardware to its potential. The warning about Mixtral 8x7B is valuable, but the overall recommendations are conservative.\"\n",
      "    },\n",
      "    {\n",
      "      \"rank\": 4,\n",
      "      \"competitor\": 4,\n",
      "      \"justification\": \"Competitor 4's response is the weakest. They suggest outdated models (Pythia-12B, LLaMA2) without providing specific Ollama commands or accurate VRAM estimates. The recommendations are vague and generic, lacking the practical specificity needed. They don't mention more current and capable options like Qwen 2.5, Llama 3.1, or provide the detailed quantization guidance that the other competitors offer. The response feels more theoretical than practical.\"\n",
      "    }\n",
      "  ],\n",
      "  \"top_3_models_overall\": [\n",
      "    {\n",
      "      \"model\": \"Qwen 2.5 32B Q4_K_M\",\n",
      "      \"justification\": \"This model strikes the perfect balance for 16GB VRAM. At ~13-14GB VRAM usage, it fits comfortably while leaving headroom for the OS and other applications. Q4_K_M quantization maintains excellent quality, and the 32B parameter count provides outstanding performance for coding, reasoning, and general tasks. This is the most practical recommendation for daily use, offering high capability without pushing hardware limits.\"\n",
      "    },\n",
      "    {\n",
      "      \"model\": \"Llama 3.1 70B Q3_K_M (or Q2_K)\",\n",
      "      \"justification\": \"For users wanting maximum capability, this represents the largest viable model for 16GB VRAM. The Q3_K_M version uses ~15.5-16GB (tight fit) while Q2_K uses ~14-15GB with slightly lower quality. Despite the aggressive quantization, the 70B parameter count often outperforms smaller models even at lower precision. This is the 'best' model in terms of raw capability, though with trade-offs in speed and VRAM headroom.\"\n",
      "    },\n",
      "    {\n",
      "      \"model\": \"Llama 3.1 8B Q6_K (or Q8_0)\",\n",
      "      \"justification\": \"For users prioritizing speed and reliability, this model fits entirely in VRAM (~5.8-8.2GB) and delivers excellent performance at 40-60+ tokens/second. Llama 3.1 8B is highly capable for its size, offering state-of-the-art performance in the small model category. The higher quantization (Q6_K or Q8_0) ensures minimal quality loss, making it ideal for interactive use cases where response time matters.\"\n",
      "    }\n",
      "  ],\n",
      "  \"qwen_2_5_32b_q3_k_m_analysis\": \"The Qwen 2.5 32B Q3_K_M variant would use approximately 11-12GB VRAM, making it very comfortable for 16GB VRAM systems. However, Q3_K_M quantization would result in noticeably lower quality compared to Q4_K_M, with more potential for degraded reasoning and accuracy. While it would be faster and leave more VRAM headroom, the Q4_K_M version is strongly preferred as it maintains significantly better quality with only 1-2GB additional VRAM usage. The Q3_K_M variant could be considered if the user needs to run other VRAM-intensive applications simultaneously or experiences thermal throttling issues with higher VRAM usage.\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(results) # ['claude-sonnet-4-5-20250929', 'gemini-2.5-flash', 'grok-4', 'qwen2.5:32b-instruct-q3_K_M']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[57]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# OK let's turn this into results!\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m results_dict = \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m ranks = results_dict[\u001b[33m\"\u001b[39m\u001b[33mresults\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m index, result \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(ranks):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\json\\__init__.py:346\u001b[39m, in \u001b[36mloads\u001b[39m\u001b[34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n\u001b[32m    341\u001b[39m     s = s.decode(detect_encoding(s), \u001b[33m'\u001b[39m\u001b[33msurrogatepass\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    344\u001b[39m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    345\u001b[39m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    348\u001b[39m     \u001b[38;5;28mcls\u001b[39m = JSONDecoder\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\json\\decoder.py:338\u001b[39m, in \u001b[36mJSONDecoder.decode\u001b[39m\u001b[34m(self, s, _w)\u001b[39m\n\u001b[32m    333\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w=WHITESPACE.match):\n\u001b[32m    334\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[32m    335\u001b[39m \u001b[33;03m    containing a JSON document).\u001b[39;00m\n\u001b[32m    336\u001b[39m \n\u001b[32m    337\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m     obj, end = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    339\u001b[39m     end = _w(s, end).end()\n\u001b[32m    340\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m end != \u001b[38;5;28mlen\u001b[39m(s):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\json\\decoder.py:356\u001b[39m, in \u001b[36mJSONDecoder.raw_decode\u001b[39m\u001b[34m(self, s, idx)\u001b[39m\n\u001b[32m    354\u001b[39m     obj, end = \u001b[38;5;28mself\u001b[39m.scan_once(s, idx)\n\u001b[32m    355\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m--> \u001b[39m\u001b[32m356\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[33m\"\u001b[39m\u001b[33mExpecting value\u001b[39m\u001b[33m\"\u001b[39m, s, err.value) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[31mJSONDecodeError\u001b[39m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "# OK let's turn this into results!\n",
    "\n",
    "results_dict = json.loads(results)\n",
    "ranks = results_dict[\"results\"]\n",
    "for index, result in enumerate(ranks):\n",
    "    competitor = competitors[int(result)-1]\n",
    "    print(f\"Rank {index+1}: {competitor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/exercise.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">Exercise</h2>\n",
    "            <span style=\"color:#ff7800;\">Which pattern(s) did this use? Try updating this to add another Agentic design pattern.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/business.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#00bfff;\">Commercial implications</h2>\n",
    "            <span style=\"color:#00bfff;\">These kinds of patterns - to send a task to multiple models, and evaluate results,\n",
    "            are common where you need to improve the quality of your LLM response. This approach can be universally applied\n",
    "            to business projects where accuracy is critical.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
